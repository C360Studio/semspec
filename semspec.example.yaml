# Semspec Example Configuration
# Copy this file to semspec.yaml and customize for your project

# Model configuration
model:
  # Default LLM model to use
  default: qwen2.5-coder:32b

  # Ollama API endpoint
  endpoint: http://localhost:11434/v1

  # Temperature for generation (0.0 = deterministic, 1.0 = creative)
  temperature: 0.2

  # Timeout for model requests
  timeout: 5m

# Repository configuration
repo:
  # Repository root path (auto-detected from git if empty)
  path: ""

# NATS configuration
nats:
  # NATS server URL (leave empty for embedded server)
  url: ""

  # Use embedded NATS server
  embedded: true

# Tool configuration
tools:
  # Allowlist of tool names (empty = allow all)
  # Example: ["file_read", "file_list"]
  allowlist: []
